{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "The goal of this notebook is to implement linear regression from scratch as practice to refresh my knowledge of the process.\n",
    "The goal of using Linear Regression is to find parameters $\\beta$ such that $X \\beta = y$ where $X$ is our matrix of features and $y$ is our target variable. \n",
    "\n",
    "The example dataset for this notebook is the boston housing dataset from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_X = boston['data']\n",
    "boston_X_with_intercept = np.concatenate([boston_X,np.ones((boston_X.shape[0],1))],axis= 1)\n",
    "boston_y = boston['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way I will implement linear regression is using ordinary least squares and minimizing the sum of squared residuals. This can be solved with the formula: \n",
    "$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\n",
    "Where $\\hat{\\beta}$ is our estimated parameters. This assumes that the columns of $X$ are linearly independent and that there exists a linear relationship between $X$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_linear_regression(X,y):\n",
    "    '''\n",
    "    Takes in a feature matrix X and a target matrix y and returns beta_hat which is the \n",
    "    estimated parameters such that X * beta_hat = y_hat \n",
    "    This function will assume that the first column of X is a column of ones that will\n",
    "    will be used to fit the intercept\n",
    "    '''\n",
    "    #the inverse of X transpose times X \n",
    "    XTX_inv =  np.linalg.inv(np.dot(X.T,X))\n",
    "    #times X transpose times y\n",
    "    beta_hat = np.dot(np.dot(XTX_inv, X.T),y)\n",
    "    return beta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will implement the cost function, the root mean square error\n",
    "$$ L(\\hat{y},y) = \\sqrt{\\frac{1}{n}\\sum_{i = 0}^n (y_i - \\hat{y_i})^2} $$\n",
    "This will be the cost function that I will use to evaluate my implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement the cost function\n",
    "def cost_function(y, y_hat):\n",
    "    mean_squares = np.mean((np.array(y) - np.array(y_hat))**2)\n",
    "    RMSE = np.sqrt(mean_squares)\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll use my implementation and see the RMSE, and compare it to scikit-learns implementation, both on the training set and a holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(boston_X_with_intercept,boston_y,test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  4.74482379421\n",
      "test error:  4.47084812241\n"
     ]
    }
   ],
   "source": [
    "# my implementation, with the RMSE on both the training and test sets\n",
    "\n",
    "beta_hat = basic_linear_regression(X_train,y_train)\n",
    "\n",
    "y_train_hat = X_train.dot(beta_hat)\n",
    "RMSE = cost_function(y_train,y_train_hat)\n",
    "print(\"training error: \",RMSE)\n",
    "\n",
    "y_test_hat = X_test.dot(beta_hat)\n",
    "RMSE_test = cost_function(y_test,y_test_hat)\n",
    "print(\"test error: \", RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error:  4.75594217932\n",
      "test error:  4.36362631944\n"
     ]
    }
   ],
   "source": [
    "#comparing to sklearn's implementation\n",
    "lr = LinearRegression()\n",
    "lr.fit(X=boston_X_with_intercept, y = boston_y)\n",
    "\n",
    "y_train_hat = lr.predict(X_train)\n",
    "RMSE = cost_function(y_train,y_train_hat)\n",
    "print(\"training error: \",RMSE)\n",
    "\n",
    "y_test_hat = lr.predict(X_test)\n",
    "RMSE_test = cost_function(y_test,y_test_hat)\n",
    "print(\"test error: \", RMSE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.1880115452782025"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, sklearns implementation did slightly better on the test set, but they have compareable RMSE, at about half of the standard deviation of the target set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_gradient(X,y,y_hat):\n",
    "    return np.dot(X.T, y - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iteration(X,y,beta, learning_rate):\n",
    "    '''\n",
    "    '''\n",
    "    y_hat = X.dot(beta)\n",
    "    gradient = np.dot(X.T, y - y_hat)\n",
    "    beta = beta - learning_rate*gradient\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
