{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "The goal of this notebook is to implement linear regression from scratch as practice to refresh my knowledge of the process.\n",
    "The goal of using Linear Regression is to find parameters $\\beta$ such that $X \\beta = y$ where $X$ is our matrix of features and $y$ is our target variable. \n",
    "\n",
    "The example dataset for this notebook is the boston housing dataset from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_X = boston['data']\n",
    "boston_X_with_intercept = np.concatenate([np.ones((boston_X.shape[0], 1)), boston_X], axis= 1)\n",
    "boston_y = boston['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way I will implement linear regression is using ordinary least squares and minimizing the sum of squared residuals. This can be solved with the formula: \n",
    "$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\n",
    "Where $\\hat{\\beta}$ is our estimated parameters. This assumes that the columns of $X$ are linearly independent and that there exists a linear relationship between $X$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_linear_regression(X, y):\n",
    "    '''\n",
    "    Takes in a feature matrix X and a target matrix y and returns beta_hat which is the \n",
    "    estimated parameters such that X * beta_hat = y_hat \n",
    "    This function will assume that the first column of X is a column of ones that will\n",
    "    will be used to fit the intercept\n",
    "    '''\n",
    "    #the inverse of X transpose times X \n",
    "    XTX_inv =  np.linalg.inv(np.dot(X.T, X))\n",
    "    #times X transpose times y\n",
    "    beta_hat = np.dot(np.dot(XTX_inv, X.T), y)\n",
    "    return beta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will implement the cost function, the root mean square error\n",
    "$$ L(\\hat{y},y) = \\sqrt{\\frac{1}{N}\\sum_{i = 1}^n (y_i - \\hat{y_i})^2} $$\n",
    "This will be the cost function that I will use to evaluate my implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement the cost function\n",
    "def cost_function(y, y_hat):\n",
    "    mean_squares = np.mean((np.array(y) - np.array(y_hat))**2)\n",
    "    RMSE = np.sqrt(mean_squares)\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll use my implementation and see the RMSE, and compare it to scikit-learns implementation, using ten fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Errors on 10 folds\n",
      "My implementation\n",
      "train RMSE:  4.67079221867\n",
      "test RMSE:  4.79290191691\n",
      "sklearn\n",
      "train RMSE:  4.67079221867\n",
      "test RMSE:  4.79290191691\n"
     ]
    }
   ],
   "source": [
    "my_im_train_error,my_im_test_error = [],[]\n",
    "sklearn_train_error, sklearn_test_error = [],[]\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle = True)\n",
    "for train_ind, test_ind in kf.split(boston_X_with_intercept):\n",
    "    X_train, X_test = boston_X_with_intercept[train_ind], boston_X_with_intercept[test_ind]\n",
    "    y_train, y_test = boston_y[train_ind], boston_y[test_ind]\n",
    "    \n",
    "    #estimates beta_hat for current fold\n",
    "    beta_hat = basic_linear_regression(X_train, y_train)\n",
    "    y_train_hat = X_train.dot(beta_hat)\n",
    "    my_im_train_error.append(cost_function(y_train, y_train_hat))\n",
    "                             \n",
    "    y_test_hat = X_test.dot(beta_hat)\n",
    "    my_im_test_error.append(cost_function(y_test, y_test_hat))\n",
    "    \n",
    "    #fit sklearns lr on current fold\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "                            \n",
    "    y_train_hat = lr.predict(X_train)\n",
    "    sklearn_train_error.append(cost_function(y_train, y_train_hat))\n",
    "    \n",
    "    y_test_hat = lr.predict(X_test)\n",
    "    sklearn_test_error.append(cost_function(y_test, y_test_hat))\n",
    "    \n",
    "#average errors across folds\n",
    "print(\"Average Errors on 10 folds\")\n",
    "print('My implementation')\n",
    "print('train RMSE: ', np.mean(my_im_train_error))\n",
    "print('test RMSE: ', np.mean(my_im_test_error))                            \n",
    "print('sklearn')\n",
    "print('train RMSE: ', np.mean(sklearn_train_error))\n",
    "print('test RMSE: ', np.mean(sklearn_test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.1880115452782025"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were exactly the same as sklearns implementation. This simple model has a RMSE of about 4.8 which is a little bit more than one half of a standard deviation on the target set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization: Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression can have a high variance, especially when fit on high dimensional data. To combat this we can restrict the model through regularization to avoid overfitting. The way I am going to implement this is through Ridge Regression. \n",
    "\n",
    "Ridge regression adds a penalty to the cost function for each coefficient in $\\hat{\\beta}$ to reduce the variance of the model.\n",
    "\n",
    "Ridge Regression changes the cost function from \n",
    "$$ \\sum_{i = 1}^m (y_i - X \\hat{\\beta})^2 $$\n",
    "\n",
    "to \n",
    "\n",
    "$$ \\sum_{i = 1}^m (y_i - X \\hat{\\beta})^2 + \\lambda \\sum_{i=1}^p \\hat{\\beta}_i^2 $$\n",
    "\n",
    "where lambda is a hyperparameter that determines how much regularization to add. When $\\lambda$ is 0 the cost function is identical to the sum of sqares, but as it increases the function will be more and more different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the choice of parameters $\\hat{\\beta}$ I will use gradient descent. Each parameter $\\hat{\\beta}_j$ will be updated by subtracting the gradient times a learning rate constant $\\alpha$ after many iterations the parameter estimations will approach the minimum. The rules for updating the parameters are:\n",
    "$$ \\hat{\\beta}_j = \\hat{\\beta}_j - \\alpha(\\sum_{i=1}^m (y_i - x_i \\hat{\\beta})x_i^{(j)} + 2\\lambda\\hat{\\beta}_j)\n",
    "$$ \n",
    "\n",
    "or for $j = 0$:\n",
    "\n",
    "$$ \\hat{\\beta}_0 = \\hat{\\beta}_0 - \\alpha(\\frac{1}{m}\\sum_{i=1}^m (y_i - x_i \\hat{\\beta}))\n",
    "$$ \n",
    "\n",
    "where $\\hat{y}_i$ is the prediction for training example $i$ and $\\hat{y}_i = x_i\\hat{\\beta}$ and $x^{(j)}$ is the $j^{th}$ column of $X$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#completes one iteration of gradient descent\n",
    "def find_gradient(X, y, beta, lmda):  \n",
    "    m = X.shape[0]\n",
    "    y_hat = np.dot(X, beta).flatten()\n",
    "    grad  = np.zeros(beta.shape)\n",
    "    for j in range(1, X.shape[1]):\n",
    "        grad[j] =  2 * np.sum( (y - y_hat) * -X[:,j]) + (2 * lmda) * (beta[j])\n",
    "    grad[0] = 2 * np.sum((y - y_hat))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#completes one iteration of gradient descent\n",
    "def iteration(X, y, beta, alpha ,lmda):\n",
    "    gradient = find_gradient(X, y, beta, lmda)\n",
    "    beta = beta  - (alpha * gradient)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def ridge_regression(X, y, num_steps, alpha, lmda):\n",
    "    beta = np.zeros((X.shape[1], 1))\n",
    "    for i in range(num_steps):\n",
    "        #if i% 10000 ==0:\n",
    "            #print(\"iteration {} RMSE: {}\".format(i,cost_function(y,np.dot(X,beta))))\n",
    "        beta = iteration(X, y, beta, alpha, lmda)\n",
    "    return beta  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.82187302e+01],\n",
       "       [  3.31551540e-02],\n",
       "       [  6.27003509e-01],\n",
       "       [  2.96270941e-01],\n",
       "       [  3.38524541e-03],\n",
       "       [  1.81526587e-02],\n",
       "       [  2.35262553e-01],\n",
       "       [  2.12712387e+00],\n",
       "       [  1.51580152e-01],\n",
       "       [  2.30129078e-01],\n",
       "       [  1.18694949e+01],\n",
       "       [  6.23760699e-01],\n",
       "       [  1.33310148e+01],\n",
       "       [  3.09819687e-01]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_regression(X_train,y_train,1000,.001,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_im_train_error, my_im_test_error = [], []\n",
    "sklearn_train_error, sklearn_test_error = [], []\n",
    "#normalizing X t\n",
    "boston_X_normalized = boston_X_with_intercept/boston_X_with_intercept.max()\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle = True)\n",
    "for train_ind, test_ind in kf.split(boston_X_with_intercept):\n",
    "    #normalizing X other than the intercept\n",
    "    #splitting into train test on this fold\n",
    "    X_train, X_test = boston_X_normalized[train_ind], boston_X_normalized[test_ind]\n",
    "    y_train, y_test = boston_y[train_ind], boston_y[test_ind]\n",
    "    \n",
    "    #estimates beta_hat for current fold\n",
    "    beta_hat = ridge_regression(X_train, y_train, 100000, .000001, .01)\n",
    "    y_train_hat = X_train.dot(beta_hat)\n",
    "    my_im_train_error.append(cost_function(y_train, y_train_hat))\n",
    "                             \n",
    "    y_test_hat = X_test.dot(beta_hat)\n",
    "    my_im_test_error.append(cost_function(y_test,y_test_hat))\n",
    "    \n",
    "    #fit sklearns lr on current fold\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "                            \n",
    "    y_train_hat = lr.predict(X_train)\n",
    "    sklearn_train_error.append(cost_function(y_train,y_train_hat))\n",
    "    \n",
    "    y_test_hat = lr.predict(X_test)\n",
    "    sklearn_test_error.append(cost_function(y_test,y_test_hat))\n",
    "    \n",
    "#average errors across folds\n",
    "print(\"Average Errors on 10 folds\")\n",
    "print('My implementation')\n",
    "print('train RMSE: ',np.mean(my_im_train_error))\n",
    "print('test RMSE: ',np.mean(my_im_test_error))                            \n",
    "print('sklearn')\n",
    "print('train RMSE: ',np.mean(sklearn_train_error))\n",
    "print('test RMSE: ',np.mean(sklearn_test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
