{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "The goal of this notebook is to implement linear regression from scratch as practice to refresh my knowledge of the process.\n",
    "The goal of using Linear Regression is to find parameters $\\beta$ such that $X \\beta = y$ where $X$ is our matrix of features and $y$ is our target variable. \n",
    "\n",
    "The example dataset for this notebook is the boston housing dataset from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_X = boston['data']\n",
    "boston_X_with_intercept = np.concatenate([boston_X,np.ones((boston_X.shape[0],1))],axis= 1)\n",
    "boston_y = boston['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way I will implement linear regression is using ordinary least squares and minimizing the sum of squared residuals. This can be solved with the formula: \n",
    "$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\n",
    "Where $\\hat{\\beta}$ is our estimated parameters. This assumes that the columns of $X$ are linearly independent and that there exists a linear relationship between $X$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_linear_regression(X,y):\n",
    "    '''\n",
    "    Takes in a feature matrix X and a target matrix y and returns beta_hat which is the \n",
    "    estimated parameters such that X * beta_hat = y_hat \n",
    "    This function will assume that the first column of X is a column of ones that will\n",
    "    will be used to fit the intercept\n",
    "    '''\n",
    "    #the inverse of X transpose times X \n",
    "    XTX_inv =  np.linalg.inv(np.dot(X.T,X))\n",
    "    #times X transpose times y\n",
    "    beta_hat = np.dot(np.dot(XTX_inv, X.T),y)\n",
    "    return beta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will implement the cost function, the root mean square error\n",
    "$$ L(\\hat{y},y) = \\sqrt{\\frac{1}{n}\\sum_{i = 0}^n (y_i - \\hat{y_i})^2} $$\n",
    "This will be the cost function that I will use to evaluate my implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement the cost function\n",
    "def cost_function(y, y_hat):\n",
    "    mean_squares = np.mean((np.array(y) - np.array(y_hat))**2)\n",
    "    RMSE = np.sqrt(mean_squares)\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll use my implementation and see the RMSE, and compare it to scikit-learns implementation, using ten fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Errors on 10 folds\n",
      "My implementation\n",
      "train RMSE:  4.67079221867\n",
      "test RMSE:  4.79290191691\n",
      "sklearn\n",
      "train RMSE:  4.67079221867\n",
      "test RMSE:  4.79290191691\n"
     ]
    }
   ],
   "source": [
    "my_im_train_error,my_im_test_error = [],[]\n",
    "sklearn_train_error, sklearn_test_error = [],[]\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle = True)\n",
    "for train_ind, test_ind in kf.split(boston_X_with_intercept):\n",
    "    X_train, X_test = boston_X_with_intercept[train_ind],boston_X_with_intercept[test_ind]\n",
    "    y_train, y_test = boston_y[train_ind],boston_y[test_ind]\n",
    "    \n",
    "    #estimates beta_hat for current fold\n",
    "    beta_hat = basic_linear_regression(X_train,y_train)\n",
    "    y_train_hat = X_train.dot(beta_hat)\n",
    "    my_im_train_error.append(cost_function(y_train,y_train_hat))\n",
    "                             \n",
    "    y_test_hat = X_test.dot(beta_hat)\n",
    "    my_im_test_error.append(cost_function(y_test,y_test_hat))\n",
    "    \n",
    "    #fit sklearns lr on current fold\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "                            \n",
    "    y_train_hat = lr.predict(X_train)\n",
    "    sklearn_train_error.append(cost_function(y_train,y_train_hat))\n",
    "    \n",
    "    y_test_hat = lr.predict(X_test)\n",
    "    sklearn_test_error.append(cost_function(y_test,y_test_hat))\n",
    "    \n",
    "#average errors across folds\n",
    "print(\"Average Errors on 10 folds\")\n",
    "print('My implementation')\n",
    "print('train RMSE: ',np.mean(my_im_train_error))\n",
    "print('test RMSE: ',np.mean(my_im_test_error))                            \n",
    "print('sklearn')\n",
    "print('train RMSE: ',np.mean(sklearn_train_error))\n",
    "print('test RMSE: ',np.mean(sklearn_test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.1880115452782025"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_y.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results were exactly the same as sklearns implementation. This simple model has a RMSE of about 4.8 which is a little bit more than one half of a standard deviation on the target set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_gradient(X,y,y_hat):\n",
    "    return np.dot(X.T, y - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iteration(X,y,beta, learning_rate):\n",
    "    '''\n",
    "    This function takes in X,y, beta and a learning rate and updates beta for one \n",
    "    iteration of gradient descent. \n",
    "    '''\n",
    "    y_hat = X.dot(beta)\n",
    "    gradient = np.dot(X.T, y - y_hat)\n",
    "    beta = beta - learning_rate*gradient\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_w_grad_descent(X,y, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
